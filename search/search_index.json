{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning for Tagging Graphic Designs The provided documentation has been created specifically for the pixel scrapper team. Project Description The goal of this project was to train a machine learning model that could accurately predict the tags associated with a graphic image. Getting Started To understand and work with our project follow these steps: Follow the setup instructions . Get acquainted with the tools and services we used by reading tools and services . Get acquainted with the system by reading the system overview . Get acquainted with the code by following the code walkthroughs .","title":"Home"},{"location":"#machine-learning-for-tagging-graphic-designs","text":"The provided documentation has been created specifically for the pixel scrapper team.","title":"Machine Learning for Tagging Graphic Designs"},{"location":"#project-description","text":"The goal of this project was to train a machine learning model that could accurately predict the tags associated with a graphic image.","title":"Project Description"},{"location":"#getting-started","text":"To understand and work with our project follow these steps: Follow the setup instructions . Get acquainted with the tools and services we used by reading tools and services . Get acquainted with the system by reading the system overview . Get acquainted with the code by following the code walkthroughs .","title":"Getting Started"},{"location":"code-walthroughs/","text":"Code Walkthroughs Lets look at how the code works. Code Walkthrough: Training This walkthrough takes you through the code used to train a single model. We will look at the files in our Jupyter Notebook called 334-label/resize-96/train.ipynb and 50-label/resize-96/train.ipynb . Note: These two files use different datasets for training, however the structure of the code is exactly the same. The training code follows these steps: Import required libraries. Set role and session. First we import some libraries that we need to use to create the training job. import sagemaker from sagemaker import get_execution_role from sagemaker.amazon.amazon_estimator import image_uris from sagemaker.image_uris import retrieve, config_for_framework role = get_execution_role() sess = sagemaker.Session() Set Data Channels. Next we specify the training inputs. In other words we tell the training job where to get our training and validation data as well as where it should send its output. First we format the S3 paths to the training and validation data. We also format the S3 path that will be used to store output. bucket = 'sagemaker-multi-label-data' prefix = 'ic-multi-label' training = 's3://{}/{}/training/'.format(bucket, prefix) validation = 's3://{}/{}/validation/'.format(bucket, prefix) output = 's3://{}/{}/output'.format(bucket, prefix) Next create create the training inputs and set the data channels. In other words we are telling the training job a little bit about the data it will use for training. For example we have to set content_type='application/x-recordio' because our dataset uses the RecordIO data format. train_data = sagemaker.inputs.TrainingInput( training, distribution='FullyReplicated', content_type='application/x-recordio', s3_data_type='S3Prefix' ) validation_data = sagemaker.inputs.TrainingInput( validation, distribution='FullyReplicated', content_type='application/x-recordio', s3_data_type='S3Prefix' ) data_channels = {'train': train_data, 'validation': validation_data} Get Training Image We also need to provide a machine learning algorithm to our training job this is called a training image. We use SageMakers built-in Image Classification algorithm for our training image. training_image = retrieve('image-classification', sess.boto_region_name) Create Estimator Next, we create an estimator object. Here we chose what type of instance the training job will use and how instances we want to run in parallel. multilabel_ic = sagemaker.estimator.Estimator( training_image, role, instance_count = 1, instance_type = 'ml.p3.2xlarge', output_path = output, sagemaker_session = sess ) Then we set the estimators hyper-parameters. The values we chose for these hyper-parameters can significantly effect our models accuracy. We may want to adjust them and re-train our models to see if we can get better accuracy. Its important to make sure the num_classes is set to the number of classes in our dataset or else the training will fail. Similarly num_training_samples must be set to the number of samples in the training data, or else the training will fail. For more information on hyper-parameters read Image Classification Hyperparameters . multilabel_ic.set_hyperparameters( num_classes = 334, num_training_samples = 116945, augmentation_type = 'crop_color_transform', epochs=5, image_shape = \"3,96,96\", learning_rate = 0.001, mini_batch_size = 256, multi_label = 1, use_weighted_loss = 1, optimizer = 'adam' ) Run Training Finally we can begin the training. We specify our data channels as the inputs for training then run the training job and wait for it to finish. multilabel_ic.fit(inputs = data_channels, logs = True) Code Walkthrough: Hyperparameter Tuning This walkthrough takes you through the code used to automatically tune a models hyperparameters. We will look at the files in our Jupyter Notebook called 334-label/resize-96/tune.ipynb and 50-label/resize-96/tune.ipynb. Creating a hyperparameter tuning job is very similar to creating a normal training job. One difference is instead of hard-coding the values of hyperparameters, we set ranges of hyper-parameter values. The hyperparameter tuning job will automatically train and re-train models adjusting the hyperparameters within the ranges we set. Set Initial Hyperparameters We provide the hyperparameters that cant be changed multilabel_ic.set_hyperparameters( num_classes=334, num_training_samples=116945, augmentation_type = 'crop_color_transform', epochs=5, image_shape = \"3,96,96\", multi_label=1, use_weighted_loss=1 ) Set Hyperparameter Ranges and Create Tuner There are 3 types of parameters: Continuous: all values in the specified range can be used. Integer: all integers in specified range can be used. Categorical: any of the values in a list can be used. Here we set the parameter ranges and create the tuner object. The max_jobs property of the tuner tells the tuning job how many different models to train with different parameters. The max_parallel_jobs property of the tuner tells the tuning job how many training jobs to run at once. tuning_job_name = \"imageclassif-job-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime())) hyperparameter_ranges = { 'learning_rate': ContinuousParameter(0.0001, 0.05), 'mini_batch_size': IntegerParameter(126, 256), 'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag'])} objective_metric_name = 'validation:accuracy' tuner = HyperparameterTuner( multilabel_ic, objective_metric_name, hyperparameter_ranges, objective_type='Maximize', max_jobs=2, max_parallel_jobs=1) Run Tuning Now we run the tuner and wait for it to finish. tuner.fit(data_channels, job_name=tuning_job_name, include_cls_metadata=False) tuner.wait() Tuner Output The models created by the tuner can be viewed and deployed from the SageMaker dashboard. The hyperparameters used for each training job can also be viewed through the SageMaker dashboard. Code Walkthrough: Creating Datasets This walkthrough will take you through the process of creating a new dataset. Requirements Local machine with at least 250 GB of free space. AWS credentials (aws_access_key_id and aws_secret_access_key) The file s3-paths.txt MXNet installed on local machine train.lst and val.lst files. Downloading the Images The first step in creating the dataset is downloading all of the images. I did this with the following files and python script: Note: aws_access_key_id and aws_secret_access_key will be omitted for privacy. Files: s3-paths.txt contains all of the images S3 locations. The script I used to download the images from S3 takes the following steps: Connect to Amazon S3 with Boto3 Read each image location from s3-paths.txt Download the image saving it in the images/ directory. Logging which images were downloaded successfully and which images were not. Downloading images script: import boto3 import botocore from botocore.retries import bucket s3_paths = open('s3-paths.txt', 'r') error_logs = open('logs.txt', 'w+') no_download = open('no-download.txt', 'w+') downloaded = open('downloaded.txt', 'w+') session = boto3.Session( aws_access_key_id=, aws_secret_access_key= ) s3 = session.resource('s3') bucket = 'pixelscrapper-user-content' for path in s3_paths: local = 'images/' + path.split('/')[-1].split('\\n')[0] try: s3.Bucket(bucket).download_file(path.split('\\n')[0], local) downloaded.write(path) except botocore.exceptions.ClientError as error: error_logs.write(str(error) + path) no_download.write(path) After this script completes we will have all of the images stored in a directory called images . Generating the Dataset After downloading the images, we need to process them to create our dataset. First we get the path of the im2rec.py tool by running the following: import mxnet path = mxnet.test_utils.get_im2rec_path() print(path) Copy the printed path and save it somewhere. We will use this path later on to run the im2rec tool. At this point we should have the following: A file called: train.lst containing the label data and local paths to training images. A file called: val.lst containing the label data and local paths to validation images. The path to im2rec.py images directory containing all downloaded images The following shows the output of running im2rec.py -h . You can see there are many options and arguments, so lets narrow down the most important ones. usage: im2rec.py [-h] [--list] [--exts EXTS [EXTS ...]] [--chunks CHUNKS] [--train-ratio TRAIN_RATIO] [--test-ratio TEST_RATIO] [--recursive] [--no-shuffle] [--pass-through] [--resize RESIZE] [--center-crop] [--quality QUALITY] [--num-thread NUM_THREAD] [--color {-1,0,1}] [--encoding {.jpg,.png}] [--pack-label] prefix root Create an image list or make a record database by reading from an image list positional arguments: prefix prefix of input/output lst and rec files. root path to folder containing images. optional arguments: -h, --help show this help message and exit Options for creating image lists: --list If this is set im2rec will create image list(s) by traversing root folder and output to <prefix>.lst. Otherwise im2rec will read <prefix>.lst and create a database at <prefix>.rec (default: False) --exts EXTS [EXTS ...] list of acceptable image extensions. (default: ['.jpeg', '.jpg', '.png']) --chunks CHUNKS number of chunks. (default: 1) --train-ratio TRAIN_RATIO Ratio of images to use for training. (default: 1.0) --test-ratio TEST_RATIO Ratio of images to use for testing. (default: 0) --recursive If true recursively walk through subdirs and assign an unique label to images in each folder. Otherwise only include images in the root folder and give them label 1. (default: False) --no-shuffle If this is passed, im2rec will not randomize the image order in <prefix>.lst (default: True) Options for creating database: --pass-through whether to skip transformation and save image as is (default: False) --resize RESIZE resize the shorter edge of image to the newsize, original images will be packed by default. (default: 0) --center-crop specify whether to crop the center image to make it rectangular. (default: False) --quality QUALITY JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9 (default: 95) --num-thread NUM_THREAD number of thread to use for encoding. order of images will be different from the input list if >1. the input list will be modified to match the resulting order. (default: 1) --color {-1,0,1} specify the color mode of the loaded image. 1: Loads a color image. Any transparency of image will be neglected. It is the default flag. 0: Loads image in grayscale mode. -1:Loads image as such including alpha channel. (default: 1) --encoding {.jpg,.png} specify the encoding of the images. (default: .jpg) --pack-label Whether to also pack multi dimensional label in the record file (default: False) We already have the training and validation list files, so we are not interested in using im2rec to create those. We are primarily concerned with using im2rec to create the databases. The most important arguments for creating our dataset using im2rec are: --resize RESIZE resize the shorter edge of image to the newsize, original images will be packed by default. (default: 0) --num-thread NUM_THREAD number of thread to use for encoding. order of images will be different from the input list if >1. the input list will be modified to match the resulting order. (default: 1) --pack-label Whether to also pack multi dimensional label in the record file (default: False) Important: you can change the resize value and number of threads in the following commands if you want. These are just examples. Create the training record by running: $ path/to/im2rec.py --resize 256 --num-thread 4 --pack-label train.lst images This will generate the files train.rec and train.idx . Next, create the validation record by running: $ path/to/im2rec.py --resize 256 --num-thread 4 --pack-label val.lst images This will generate the files val.rec and val.idx . train.rec and val.rec are now ready to be uploaded to Amazon S3 where they can be used as training inputs.","title":"Code Walkthroughs"},{"location":"code-walthroughs/#code-walkthroughs","text":"Lets look at how the code works.","title":"Code Walkthroughs"},{"location":"code-walthroughs/#code-walkthrough-training","text":"This walkthrough takes you through the code used to train a single model. We will look at the files in our Jupyter Notebook called 334-label/resize-96/train.ipynb and 50-label/resize-96/train.ipynb . Note: These two files use different datasets for training, however the structure of the code is exactly the same. The training code follows these steps:","title":"Code Walkthrough: Training"},{"location":"code-walthroughs/#import-required-libraries-set-role-and-session","text":"First we import some libraries that we need to use to create the training job. import sagemaker from sagemaker import get_execution_role from sagemaker.amazon.amazon_estimator import image_uris from sagemaker.image_uris import retrieve, config_for_framework role = get_execution_role() sess = sagemaker.Session()","title":"Import required libraries. Set role and session."},{"location":"code-walthroughs/#set-data-channels","text":"Next we specify the training inputs. In other words we tell the training job where to get our training and validation data as well as where it should send its output. First we format the S3 paths to the training and validation data. We also format the S3 path that will be used to store output. bucket = 'sagemaker-multi-label-data' prefix = 'ic-multi-label' training = 's3://{}/{}/training/'.format(bucket, prefix) validation = 's3://{}/{}/validation/'.format(bucket, prefix) output = 's3://{}/{}/output'.format(bucket, prefix) Next create create the training inputs and set the data channels. In other words we are telling the training job a little bit about the data it will use for training. For example we have to set content_type='application/x-recordio' because our dataset uses the RecordIO data format. train_data = sagemaker.inputs.TrainingInput( training, distribution='FullyReplicated', content_type='application/x-recordio', s3_data_type='S3Prefix' ) validation_data = sagemaker.inputs.TrainingInput( validation, distribution='FullyReplicated', content_type='application/x-recordio', s3_data_type='S3Prefix' ) data_channels = {'train': train_data, 'validation': validation_data}","title":"Set Data Channels."},{"location":"code-walthroughs/#get-training-image","text":"We also need to provide a machine learning algorithm to our training job this is called a training image. We use SageMakers built-in Image Classification algorithm for our training image. training_image = retrieve('image-classification', sess.boto_region_name)","title":"Get Training Image"},{"location":"code-walthroughs/#create-estimator","text":"Next, we create an estimator object. Here we chose what type of instance the training job will use and how instances we want to run in parallel. multilabel_ic = sagemaker.estimator.Estimator( training_image, role, instance_count = 1, instance_type = 'ml.p3.2xlarge', output_path = output, sagemaker_session = sess ) Then we set the estimators hyper-parameters. The values we chose for these hyper-parameters can significantly effect our models accuracy. We may want to adjust them and re-train our models to see if we can get better accuracy. Its important to make sure the num_classes is set to the number of classes in our dataset or else the training will fail. Similarly num_training_samples must be set to the number of samples in the training data, or else the training will fail. For more information on hyper-parameters read Image Classification Hyperparameters . multilabel_ic.set_hyperparameters( num_classes = 334, num_training_samples = 116945, augmentation_type = 'crop_color_transform', epochs=5, image_shape = \"3,96,96\", learning_rate = 0.001, mini_batch_size = 256, multi_label = 1, use_weighted_loss = 1, optimizer = 'adam' )","title":"Create Estimator"},{"location":"code-walthroughs/#run-training","text":"Finally we can begin the training. We specify our data channels as the inputs for training then run the training job and wait for it to finish. multilabel_ic.fit(inputs = data_channels, logs = True)","title":"Run Training"},{"location":"code-walthroughs/#code-walkthrough-hyperparameter-tuning","text":"This walkthrough takes you through the code used to automatically tune a models hyperparameters. We will look at the files in our Jupyter Notebook called 334-label/resize-96/tune.ipynb and 50-label/resize-96/tune.ipynb. Creating a hyperparameter tuning job is very similar to creating a normal training job. One difference is instead of hard-coding the values of hyperparameters, we set ranges of hyper-parameter values. The hyperparameter tuning job will automatically train and re-train models adjusting the hyperparameters within the ranges we set.","title":"Code Walkthrough: Hyperparameter Tuning"},{"location":"code-walthroughs/#set-initial-hyperparameters","text":"We provide the hyperparameters that cant be changed multilabel_ic.set_hyperparameters( num_classes=334, num_training_samples=116945, augmentation_type = 'crop_color_transform', epochs=5, image_shape = \"3,96,96\", multi_label=1, use_weighted_loss=1 )","title":"Set Initial Hyperparameters"},{"location":"code-walthroughs/#set-hyperparameter-ranges-and-create-tuner","text":"There are 3 types of parameters: Continuous: all values in the specified range can be used. Integer: all integers in specified range can be used. Categorical: any of the values in a list can be used. Here we set the parameter ranges and create the tuner object. The max_jobs property of the tuner tells the tuning job how many different models to train with different parameters. The max_parallel_jobs property of the tuner tells the tuning job how many training jobs to run at once. tuning_job_name = \"imageclassif-job-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime())) hyperparameter_ranges = { 'learning_rate': ContinuousParameter(0.0001, 0.05), 'mini_batch_size': IntegerParameter(126, 256), 'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'nag'])} objective_metric_name = 'validation:accuracy' tuner = HyperparameterTuner( multilabel_ic, objective_metric_name, hyperparameter_ranges, objective_type='Maximize', max_jobs=2, max_parallel_jobs=1)","title":"Set Hyperparameter Ranges and Create Tuner"},{"location":"code-walthroughs/#run-tuning","text":"Now we run the tuner and wait for it to finish. tuner.fit(data_channels, job_name=tuning_job_name, include_cls_metadata=False) tuner.wait()","title":"Run Tuning"},{"location":"code-walthroughs/#tuner-output","text":"The models created by the tuner can be viewed and deployed from the SageMaker dashboard. The hyperparameters used for each training job can also be viewed through the SageMaker dashboard.","title":"Tuner Output"},{"location":"code-walthroughs/#code-walkthrough-creating-datasets","text":"This walkthrough will take you through the process of creating a new dataset.","title":"Code Walkthrough: Creating Datasets"},{"location":"code-walthroughs/#requirements","text":"Local machine with at least 250 GB of free space. AWS credentials (aws_access_key_id and aws_secret_access_key) The file s3-paths.txt MXNet installed on local machine train.lst and val.lst files.","title":"Requirements"},{"location":"code-walthroughs/#downloading-the-images","text":"The first step in creating the dataset is downloading all of the images. I did this with the following files and python script: Note: aws_access_key_id and aws_secret_access_key will be omitted for privacy. Files: s3-paths.txt contains all of the images S3 locations. The script I used to download the images from S3 takes the following steps: Connect to Amazon S3 with Boto3 Read each image location from s3-paths.txt Download the image saving it in the images/ directory. Logging which images were downloaded successfully and which images were not. Downloading images script: import boto3 import botocore from botocore.retries import bucket s3_paths = open('s3-paths.txt', 'r') error_logs = open('logs.txt', 'w+') no_download = open('no-download.txt', 'w+') downloaded = open('downloaded.txt', 'w+') session = boto3.Session( aws_access_key_id=, aws_secret_access_key= ) s3 = session.resource('s3') bucket = 'pixelscrapper-user-content' for path in s3_paths: local = 'images/' + path.split('/')[-1].split('\\n')[0] try: s3.Bucket(bucket).download_file(path.split('\\n')[0], local) downloaded.write(path) except botocore.exceptions.ClientError as error: error_logs.write(str(error) + path) no_download.write(path) After this script completes we will have all of the images stored in a directory called images .","title":"Downloading the Images"},{"location":"code-walthroughs/#generating-the-dataset","text":"After downloading the images, we need to process them to create our dataset. First we get the path of the im2rec.py tool by running the following: import mxnet path = mxnet.test_utils.get_im2rec_path() print(path) Copy the printed path and save it somewhere. We will use this path later on to run the im2rec tool. At this point we should have the following: A file called: train.lst containing the label data and local paths to training images. A file called: val.lst containing the label data and local paths to validation images. The path to im2rec.py images directory containing all downloaded images The following shows the output of running im2rec.py -h . You can see there are many options and arguments, so lets narrow down the most important ones. usage: im2rec.py [-h] [--list] [--exts EXTS [EXTS ...]] [--chunks CHUNKS] [--train-ratio TRAIN_RATIO] [--test-ratio TEST_RATIO] [--recursive] [--no-shuffle] [--pass-through] [--resize RESIZE] [--center-crop] [--quality QUALITY] [--num-thread NUM_THREAD] [--color {-1,0,1}] [--encoding {.jpg,.png}] [--pack-label] prefix root Create an image list or make a record database by reading from an image list positional arguments: prefix prefix of input/output lst and rec files. root path to folder containing images. optional arguments: -h, --help show this help message and exit Options for creating image lists: --list If this is set im2rec will create image list(s) by traversing root folder and output to <prefix>.lst. Otherwise im2rec will read <prefix>.lst and create a database at <prefix>.rec (default: False) --exts EXTS [EXTS ...] list of acceptable image extensions. (default: ['.jpeg', '.jpg', '.png']) --chunks CHUNKS number of chunks. (default: 1) --train-ratio TRAIN_RATIO Ratio of images to use for training. (default: 1.0) --test-ratio TEST_RATIO Ratio of images to use for testing. (default: 0) --recursive If true recursively walk through subdirs and assign an unique label to images in each folder. Otherwise only include images in the root folder and give them label 1. (default: False) --no-shuffle If this is passed, im2rec will not randomize the image order in <prefix>.lst (default: True) Options for creating database: --pass-through whether to skip transformation and save image as is (default: False) --resize RESIZE resize the shorter edge of image to the newsize, original images will be packed by default. (default: 0) --center-crop specify whether to crop the center image to make it rectangular. (default: False) --quality QUALITY JPEG quality for encoding, 1-100; or PNG compression for encoding, 1-9 (default: 95) --num-thread NUM_THREAD number of thread to use for encoding. order of images will be different from the input list if >1. the input list will be modified to match the resulting order. (default: 1) --color {-1,0,1} specify the color mode of the loaded image. 1: Loads a color image. Any transparency of image will be neglected. It is the default flag. 0: Loads image in grayscale mode. -1:Loads image as such including alpha channel. (default: 1) --encoding {.jpg,.png} specify the encoding of the images. (default: .jpg) --pack-label Whether to also pack multi dimensional label in the record file (default: False) We already have the training and validation list files, so we are not interested in using im2rec to create those. We are primarily concerned with using im2rec to create the databases. The most important arguments for creating our dataset using im2rec are: --resize RESIZE resize the shorter edge of image to the newsize, original images will be packed by default. (default: 0) --num-thread NUM_THREAD number of thread to use for encoding. order of images will be different from the input list if >1. the input list will be modified to match the resulting order. (default: 1) --pack-label Whether to also pack multi dimensional label in the record file (default: False) Important: you can change the resize value and number of threads in the following commands if you want. These are just examples. Create the training record by running: $ path/to/im2rec.py --resize 256 --num-thread 4 --pack-label train.lst images This will generate the files train.rec and train.idx . Next, create the validation record by running: $ path/to/im2rec.py --resize 256 --num-thread 4 --pack-label val.lst images This will generate the files val.rec and val.idx . train.rec and val.rec are now ready to be uploaded to Amazon S3 where they can be used as training inputs.","title":"Generating the Dataset"},{"location":"resources/","text":"Addition Resources A big list of helpful resources. Additional Resources: Video: Overview of SageMaker Create a Notebook Instance Official Amazon SageMaker Documentation Official Amazon S3 documentation AWS documentation Additional Resources: Using im2rec.py to create a dataset MXNet RecordIO documentation Additional Resources: Code Walkthrough: training Video: Overview of SageMaker Create a Notebook Instance Official Amazon SageMaker Documentation Additional Resources: Deploy a Model to an Endpoint in SageMaker","title":"Additional Resources"},{"location":"resources/#addition-resources","text":"A big list of helpful resources. Additional Resources: Video: Overview of SageMaker Create a Notebook Instance Official Amazon SageMaker Documentation Official Amazon S3 documentation AWS documentation Additional Resources: Using im2rec.py to create a dataset MXNet RecordIO documentation Additional Resources: Code Walkthrough: training Video: Overview of SageMaker Create a Notebook Instance Official Amazon SageMaker Documentation Additional Resources: Deploy a Model to an Endpoint in SageMaker","title":"Addition Resources"},{"location":"setup/","text":"Setup Instructions Follow these steps to get started: Log in to AWS Navigate to the Amazon SageMaker dashboard. Select Notebook instances, then start the instance named sagemaker-notebook . Once the instance is started select Open Jupyter . You will be redirected to the Jupyter Notebook. After completing these steps you can run training jobs from the jupyter notebook. Next Steps: Read about the tools and services we used to create models. Follow the code walkthroughs to learn how the code in the Jupyter notebook works.","title":"Setup Instructions"},{"location":"setup/#setup-instructions","text":"Follow these steps to get started: Log in to AWS Navigate to the Amazon SageMaker dashboard. Select Notebook instances, then start the instance named sagemaker-notebook . Once the instance is started select Open Jupyter . You will be redirected to the Jupyter Notebook. After completing these steps you can run training jobs from the jupyter notebook. Next Steps: Read about the tools and services we used to create models. Follow the code walkthroughs to learn how the code in the Jupyter notebook works.","title":"Setup Instructions"},{"location":"system/","text":"System Overview Overview of the components and structures of our project. Structure of S3 Bucket Containing Datasets Datasets are stored in an S3 bucket named: sagemaker-multi-label-data The following shows the file structure of this bucket: sagemaker-multi-label-data/ 50-label-rs96/ training/ train.rec validation/ val.rec output/ training outputs ic-multi-label training/ train.rec validation/ val.rec output/ training outputs There are two datasets with different properties in this bucket. This table shows the properties of each of the datasets: .tg {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;} .tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} Bucket Prefix # of Labels # Training Samples Image Size Pixels sagemaker-multi-label-data 50-label-rs96 50 98378 96x96 sagemaker-multi-label-data ic-multi-label 334 116945 96x96 Training models with either of these datasets can be done through the files in our Jupyter Notebook. Structure of Jupyter Notebook The following shows the file structure of the Jupyter notebook: machine-learning-for-taggging-graphic-designs/ 334-label/ resize-96/ train.ipynb tune.ipynb 50-label/ resize-96/ train.ipynb tune.ipynb This table shows what dataset is used by what Jupyter Notebook file: .tg {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;} .tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0lax{text-align:left;vertical-align:top} Location of File in Jupyter Notebook S3 Location of Dataset Used by that file 334-label/resize-96/train.ipynb 334-label/resize-96/tune.ipynb sagemaker-multi-label-data/ic-multi-label/ 50-label/resize-96/train.ipynb 50-label/resize-96/tune.ipynb sagemaker-multi-label-data/50-label-rs96/ As you can see different files in the Jupyter Notebook use different datasets. This means that we are able to train a model with either 50-label images, or 334-label images. Data Overview Images The raw data consists of ~155,000 classified images stored in S3. Each image can belong to one, or more of 334 classes. List Files List files includes the label data for each image as well as the path to the image. We use the list file to associate the images and their classes. Each line in the list file takes the following format: Image-ID label_1 label_2 ... label_334 Image-Path The labels represent the classes that images can belong to. Label values can either be 0, meaning the image does not belong to that class, or 1 meaning the image does belong to that class. For example, lets say we have 3 images that can belong to 3 classes: flower, leaf, and blue. The list file would look like this: 0 1 0 0 image1.jpg 1 0 1 1 image2.jpg 2 1 1 1 image3.jpg The first column is a unique ID for each image. The next 3 columns are the images label data and the last column is the path of the image. Lets add the column headers in for the sake of understanding our example: ID flower leaf blue path 0 1 0 0 image1.jpg 1 0 1 1 image2.jpg 2 1 1 1 image3.jpg Note: The column headers would not be included in the List file. We can see that the first label is flower, the second is leaf and the third is blue. So, image1.jpg belongs to the class flower only, image2.jpg is belongs to both the leaf and blue class and image3.jpg belongs to all three classes. List files are a crucial part of processing the data. They contain the label data associated with each image. Each image and its label data will be packed into the final format of the training dataset. Data Processing Using the raw images for training is possible, but inefficient. Processing the images allows for faster training and less resource consumption. Processing the images involves the following steps: Download images from S3. Create dataset using RecordIO. Downloading Images From S3 We were provided a list file containing the label data and S3 location of every image: ID labels 1-334 Path to image in S3 To download the images we used a python script to parse each line of the original list file, then write the S3 location of each image into a file called s3-paths.txt . Then, using a tool called boto3, we connect to S3 and download every image to a local machine. For more information on Boto3 see the Tools and Services: Boto3 . Check out Code Walkthrough: Downloading the Images to see how we downloaded the images. Create Dataset Using RecordIO We use MXNet's RecordIO data format for the dataset. This provides many benefits such as: reducing training time, reducing overall size of the dataset and more. We have to convert the raw images and the label data into the RecordIO data format. To do this we use a tool called im2rec.py provided by MXNet. This tool takes our list file and the root directory containing our images and produces a .rec dataset. The .rec file that im2rec.py generates can be used as training data. We upload the dataset back to S3 and we are ready to use it for training. Check out Code Walkthrough: Creating Datasets to see how we created the datasets in the RecordIO format. Model Training Training a machine Learning model with SageMaker is done through the SageMaker Notebook instance. Within the instance we write code to setup and run training jobs. Training jobs in SageMaker include the following information: Location of training data. The training data is stored in an Amazon Simple Storage bucket. The URL of the bucket containing the training data is provided to the training job before launching it. Type of compute instance to use for training. Training models is done in a separate compute instance. There are several types of compute instances that can be used for training. The desired type of compute instance is provided to the training job before launching it. Location of where the output should be stored. The URL of the bucket where outputs from a training job should be stored. Path of EC2 container registry where training code is stored. This path contains an image of the actual training algorithm. SageMaker provides several built-in training images that can be used. The path of the container where the training image is stored is provided to the training job before launching it. We write code in our notebook instance to specify all of this information before launching the training job. Additionally, we specify some model parameters that describe how the training should take place. Check out Code Walkthrough: Training and Code Walkthrough: Hyperparameter Tuning to see how we trained models from our Jupyter notebook. Model Deployment Trained models can be deployed to endpoints with sagemaker. The endpoint can then receive input data from a client application and pass it to the trained model. The models output is then returned back to the client application through the endpoint.","title":"System Overview"},{"location":"system/#system-overview","text":"Overview of the components and structures of our project.","title":"System Overview"},{"location":"system/#structure-of-s3-bucket-containing-datasets","text":"Datasets are stored in an S3 bucket named: sagemaker-multi-label-data The following shows the file structure of this bucket: sagemaker-multi-label-data/ 50-label-rs96/ training/ train.rec validation/ val.rec output/ training outputs ic-multi-label training/ train.rec validation/ val.rec output/ training outputs There are two datasets with different properties in this bucket. This table shows the properties of each of the datasets: .tg {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;} .tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top} .tg .tg-0lax{text-align:left;vertical-align:top} Bucket Prefix # of Labels # Training Samples Image Size Pixels sagemaker-multi-label-data 50-label-rs96 50 98378 96x96 sagemaker-multi-label-data ic-multi-label 334 116945 96x96 Training models with either of these datasets can be done through the files in our Jupyter Notebook.","title":"Structure of S3 Bucket Containing Datasets"},{"location":"system/#structure-of-jupyter-notebook","text":"The following shows the file structure of the Jupyter notebook: machine-learning-for-taggging-graphic-designs/ 334-label/ resize-96/ train.ipynb tune.ipynb 50-label/ resize-96/ train.ipynb tune.ipynb This table shows what dataset is used by what Jupyter Notebook file: .tg {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;} .tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36; font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3; font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-0lax{text-align:left;vertical-align:top} Location of File in Jupyter Notebook S3 Location of Dataset Used by that file 334-label/resize-96/train.ipynb 334-label/resize-96/tune.ipynb sagemaker-multi-label-data/ic-multi-label/ 50-label/resize-96/train.ipynb 50-label/resize-96/tune.ipynb sagemaker-multi-label-data/50-label-rs96/ As you can see different files in the Jupyter Notebook use different datasets. This means that we are able to train a model with either 50-label images, or 334-label images.","title":"Structure of Jupyter Notebook"},{"location":"system/#data-overview","text":"","title":"Data Overview"},{"location":"system/#images","text":"The raw data consists of ~155,000 classified images stored in S3. Each image can belong to one, or more of 334 classes.","title":"Images"},{"location":"system/#list-files","text":"List files includes the label data for each image as well as the path to the image. We use the list file to associate the images and their classes. Each line in the list file takes the following format: Image-ID label_1 label_2 ... label_334 Image-Path The labels represent the classes that images can belong to. Label values can either be 0, meaning the image does not belong to that class, or 1 meaning the image does belong to that class. For example, lets say we have 3 images that can belong to 3 classes: flower, leaf, and blue. The list file would look like this: 0 1 0 0 image1.jpg 1 0 1 1 image2.jpg 2 1 1 1 image3.jpg The first column is a unique ID for each image. The next 3 columns are the images label data and the last column is the path of the image. Lets add the column headers in for the sake of understanding our example: ID flower leaf blue path 0 1 0 0 image1.jpg 1 0 1 1 image2.jpg 2 1 1 1 image3.jpg Note: The column headers would not be included in the List file. We can see that the first label is flower, the second is leaf and the third is blue. So, image1.jpg belongs to the class flower only, image2.jpg is belongs to both the leaf and blue class and image3.jpg belongs to all three classes. List files are a crucial part of processing the data. They contain the label data associated with each image. Each image and its label data will be packed into the final format of the training dataset.","title":"List Files"},{"location":"system/#data-processing","text":"Using the raw images for training is possible, but inefficient. Processing the images allows for faster training and less resource consumption. Processing the images involves the following steps: Download images from S3. Create dataset using RecordIO.","title":"Data Processing"},{"location":"system/#downloading-images-from-s3","text":"We were provided a list file containing the label data and S3 location of every image: ID labels 1-334 Path to image in S3 To download the images we used a python script to parse each line of the original list file, then write the S3 location of each image into a file called s3-paths.txt . Then, using a tool called boto3, we connect to S3 and download every image to a local machine. For more information on Boto3 see the Tools and Services: Boto3 . Check out Code Walkthrough: Downloading the Images to see how we downloaded the images.","title":"Downloading Images From S3"},{"location":"system/#create-dataset-using-recordio","text":"We use MXNet's RecordIO data format for the dataset. This provides many benefits such as: reducing training time, reducing overall size of the dataset and more. We have to convert the raw images and the label data into the RecordIO data format. To do this we use a tool called im2rec.py provided by MXNet. This tool takes our list file and the root directory containing our images and produces a .rec dataset. The .rec file that im2rec.py generates can be used as training data. We upload the dataset back to S3 and we are ready to use it for training. Check out Code Walkthrough: Creating Datasets to see how we created the datasets in the RecordIO format.","title":"Create Dataset Using RecordIO"},{"location":"system/#model-training","text":"Training a machine Learning model with SageMaker is done through the SageMaker Notebook instance. Within the instance we write code to setup and run training jobs. Training jobs in SageMaker include the following information: Location of training data. The training data is stored in an Amazon Simple Storage bucket. The URL of the bucket containing the training data is provided to the training job before launching it. Type of compute instance to use for training. Training models is done in a separate compute instance. There are several types of compute instances that can be used for training. The desired type of compute instance is provided to the training job before launching it. Location of where the output should be stored. The URL of the bucket where outputs from a training job should be stored. Path of EC2 container registry where training code is stored. This path contains an image of the actual training algorithm. SageMaker provides several built-in training images that can be used. The path of the container where the training image is stored is provided to the training job before launching it. We write code in our notebook instance to specify all of this information before launching the training job. Additionally, we specify some model parameters that describe how the training should take place. Check out Code Walkthrough: Training and Code Walkthrough: Hyperparameter Tuning to see how we trained models from our Jupyter notebook.","title":"Model Training"},{"location":"system/#model-deployment","text":"Trained models can be deployed to endpoints with sagemaker. The endpoint can then receive input data from a client application and pass it to the trained model. The models output is then returned back to the client application through the endpoint.","title":"Model Deployment"},{"location":"tands/","text":"Tools and Services General information about some of the tools and services used in this project. Amazon Web Services (AWS) Amazon Web services (AWS) is a cloud computing platform that we used to produce our trained model. We used two AWS services: Amazon Simple Storage Amazon SageMaker Lets take a look at these services in detail. Amazon Simple Storage (Amazon S3): Amazon Simple Storage (Amazon S3) provides cloud-based data storage. We use Amazon S3 to store our training and validation data. After creating our processed datasets we upload them to S3. When training a model these datasets will be fetched from S3. Amazon SageMaker: Amazon SageMaker is a machine learning service that allows developers to train tune and deploy models. Notebook Instances: SageMaker Notebook Instances are machine learning compute instances that run the Jupyter Notebook App . These instances allow us to do a variety of tasks related to the production of machine learning models. After creating a Notebook Instance we can open a Jupyter Notebook within it. The Jupyter Notebook is where we write our code to: setup, run and evaluate training jobs. Check out Code Walkthrough: Training and Code Walkthrough: Hyperparameter Tuning to see how we trained models from our Jupyter notebook. MXNet MXNet is an open source deep learning library. We used one specific tool provided by the MXNet library called im2rec. This tool allowed us to create our datasets in the RecordIO data format. For more information about MXNet's RecordIO data format and the im2rec tool see Creating a Dataset Using RecordIO . Also check out Code Walkthrough: Creating Datasets to see how we created the datasets in the RecordIO format. Boto3 We used the Boto3 AWS SDK to connect python scripts to Amazon S3 and download images. Check out Code Walkthrough: Downloading the Images to see how we used boto3 to download images. Also check out the official boto3 documentation for more information.","title":"Tools and Services"},{"location":"tands/#tools-and-services","text":"General information about some of the tools and services used in this project.","title":"Tools and Services"},{"location":"tands/#amazon-web-services-aws","text":"Amazon Web services (AWS) is a cloud computing platform that we used to produce our trained model. We used two AWS services: Amazon Simple Storage Amazon SageMaker Lets take a look at these services in detail.","title":"Amazon Web Services (AWS)"},{"location":"tands/#amazon-simple-storage-amazon-s3","text":"Amazon Simple Storage (Amazon S3) provides cloud-based data storage. We use Amazon S3 to store our training and validation data. After creating our processed datasets we upload them to S3. When training a model these datasets will be fetched from S3.","title":"Amazon Simple Storage (Amazon S3):"},{"location":"tands/#amazon-sagemaker","text":"Amazon SageMaker is a machine learning service that allows developers to train tune and deploy models.","title":"Amazon SageMaker:"},{"location":"tands/#notebook-instances","text":"SageMaker Notebook Instances are machine learning compute instances that run the Jupyter Notebook App . These instances allow us to do a variety of tasks related to the production of machine learning models. After creating a Notebook Instance we can open a Jupyter Notebook within it. The Jupyter Notebook is where we write our code to: setup, run and evaluate training jobs. Check out Code Walkthrough: Training and Code Walkthrough: Hyperparameter Tuning to see how we trained models from our Jupyter notebook.","title":"Notebook Instances:"},{"location":"tands/#mxnet","text":"MXNet is an open source deep learning library. We used one specific tool provided by the MXNet library called im2rec. This tool allowed us to create our datasets in the RecordIO data format. For more information about MXNet's RecordIO data format and the im2rec tool see Creating a Dataset Using RecordIO . Also check out Code Walkthrough: Creating Datasets to see how we created the datasets in the RecordIO format.","title":"MXNet"},{"location":"tands/#boto3","text":"We used the Boto3 AWS SDK to connect python scripts to Amazon S3 and download images. Check out Code Walkthrough: Downloading the Images to see how we used boto3 to download images. Also check out the official boto3 documentation for more information.","title":"Boto3"}]}